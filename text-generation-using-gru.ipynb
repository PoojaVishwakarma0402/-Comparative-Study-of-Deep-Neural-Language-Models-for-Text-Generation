{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7297075,"sourceType":"datasetVersion","datasetId":4232737},{"sourceId":7302768,"sourceType":"datasetVersion","datasetId":4236811}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import absolute_import, division,print_function, unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import GRU\n\nfrom keras.optimizers import RMSprop\n\nfrom keras.callbacks import LambdaCallback\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import ReduceLROnPlateau\nimport random\nimport sys\n","metadata":{"id":"ue2ZjviZ2_wa","execution":{"iopub.status.busy":"2024-01-01T09:02:38.971823Z","iopub.execute_input":"2024-01-01T09:02:38.972470Z","iopub.status.idle":"2024-01-01T09:02:42.584971Z","shell.execute_reply.started":"2024-01-01T09:02:38.972436Z","shell.execute_reply":"2024-01-01T09:02:42.583595Z"},"editable":false,"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import urllib.request\n\n# URL of the Shakespeare dataset\nurl = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n\n# Download the file\nurllib.request.urlretrieve(url, 'shakespeare.txt')\n\n# Open and read the downloaded file\nwith open('/kaggle/input/shakespeare-txt/shakespeare.txt', 'r') as file:\n    text = file.read()\n\n# A preview of the text file\nprint(text[:300])  # Displaying the first 300 characters as an example\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm2_eDGr3VJi","outputId":"0617ebe3-5857-4f75-fb47-5d507488418b","execution":{"iopub.status.busy":"2024-01-01T09:02:42.587161Z","iopub.execute_input":"2024-01-01T09:02:42.588126Z","iopub.status.idle":"2024-01-01T09:02:42.711357Z","shell.execute_reply.started":"2024-01-01T09:02:42.588090Z","shell.execute_reply":"2024-01-01T09:02:42.710303Z"},"editable":false,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"This is the 100th Etext file presented by Project Gutenberg, and\nis presented in cooperation with World Library, Inc., from their\nLibrary of the Future and Shakespeare CDROMS.  Project Gutenberg\noften releases Etexts that are NOT placed in the Public Domain!!\n\nShakespeare\n\n*This Etext has certain co\n","output_type":"stream"}]},{"cell_type":"markdown","source":" Creating a mapping from each unique character in the text to a unique number\n\n","metadata":{"id":"OF8_NzIi3zGn","editable":false}},{"cell_type":"code","source":"# Storing all the unique characters present in the text\nvocabulary = sorted(list(set(text)))\n\n# Creating dictionaries to map each character to an index\nchar_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\nindices_to_char = dict((i, c) for i, c in enumerate(vocabulary))\n\nprint(vocabulary)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ezb0LzUy3w0D","outputId":"f24cef0d-26c2-4496-eb7b-d1ad275b11ba","execution":{"iopub.status.busy":"2024-01-01T09:02:42.712712Z","iopub.execute_input":"2024-01-01T09:02:42.713015Z","iopub.status.idle":"2024-01-01T09:02:42.799705Z","shell.execute_reply.started":"2024-01-01T09:02:42.712988Z","shell.execute_reply":"2024-01-01T09:02:42.798646Z"},"editable":false,"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['\\n', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '~']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Pre-processing the data","metadata":{"id":"A6qN8fmW37qM","editable":false}},{"cell_type":"code","source":"# Dividing the text into subsequences of length max_length\n# So that at each time step the next max_length characters\n# are fed into the network\nmax_length = 50\nsteps = 5\nsentences = []\nnext_chars = []\nfor i in range(0, len(text) - max_length, steps):\n\tsentences.append(text[i: i + max_length])\n\tnext_chars.append(text[i + max_length])\n\n# Hot encoding each character into a boolean vector\n\n# Initializing a matrix of boolean vectors with each column representing\n# the hot encoded representation of the character\nX = np.zeros((len(sentences), max_length, len(vocabulary)), dtype = bool)\ny = np.zeros((len(sentences), len(vocabulary)), dtype = bool)\n\n# Placing the value 1 at the appropriate position for each vector\n# to complete the hot-encoding process\nfor i, sentence in enumerate(sentences):\n\tfor t, char in enumerate(sentence):\n\t\tX[i, t, char_to_indices[char]] = 1\n\ty[i, char_to_indices[next_chars[i]]] = 1\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnEVKRc2350Z","outputId":"97eaf0d4-b4ad-423d-80f3-14ef803816cd","execution":{"iopub.status.busy":"2024-01-01T09:02:42.803062Z","iopub.execute_input":"2024-01-01T09:02:42.803408Z","iopub.status.idle":"2024-01-01T09:03:13.960343Z","shell.execute_reply.started":"2024-01-01T09:02:42.803376Z","shell.execute_reply":"2024-01-01T09:03:13.959458Z"},"editable":false,"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Building the GRU network","metadata":{"id":"tKwrn6aD4EcO","editable":false}},{"cell_type":"code","source":"# Initializing the GRU network\nmodel = Sequential()\n\n# Defining the cell type\nmodel.add(GRU(128, input_shape =(max_length, len(vocabulary))))\n\n# Defining the densely connected Neural Network layer\nmodel.add(Dense(len(vocabulary)))\n\n# Defining the activation function for the cell\nmodel.add(Activation('softmax'))\n\n# Defining the optimizing function\noptimizer = RMSprop(lr = 0.01)\n\n# Configuring the model for training\nmodel.compile(loss ='categorical_crossentropy', optimizer = optimizer)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1aU39AH4FC3","outputId":"0982444e-d19e-47d7-c9c7-d588511e6b9a","execution":{"iopub.status.busy":"2024-01-01T09:03:13.962114Z","iopub.execute_input":"2024-01-01T09:03:13.962495Z","iopub.status.idle":"2024-01-01T09:03:16.399802Z","shell.execute_reply.started":"2024-01-01T09:03:13.962458Z","shell.execute_reply":"2024-01-01T09:03:16.398952Z"},"editable":false,"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"a) Helper function to sample the next character:\n\n","metadata":{"id":"VRzHgfsn4XY4","editable":false}},{"cell_type":"code","source":"# Helper function to sample an index from a probability array\ndef sample_index(preds, temperature = 1.0):\n# temperature determines the freedom the function has when generating text\n\n\t# Converting the predictions vector into a numpy array\n\tpreds = np.asarray(preds).astype('float64')\n\n\t# Normalizing the predictions array\n\tpreds = np.log(preds) / temperature\n\texp_preds = np.exp(preds)\n\tpreds = exp_preds / np.sum(exp_preds)\n\n\t# The main sampling step. Creates an array of probabilities signifying\n\t# the probability of each character to be the next character in the\n\t# generated text\n\tprobas = np.random.multinomial(1, preds, 1)\n\n\t# Returning the character with maximum probability to be the next character\n\t# in the generated text\n\treturn np.argmax(probas)\n","metadata":{"id":"GyPz7lFu4X78","execution":{"iopub.status.busy":"2024-01-01T09:03:16.401234Z","iopub.execute_input":"2024-01-01T09:03:16.401599Z","iopub.status.idle":"2024-01-01T09:03:16.408122Z","shell.execute_reply.started":"2024-01-01T09:03:16.401569Z","shell.execute_reply":"2024-01-01T09:03:16.407151Z"},"editable":false,"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"b) Helper function to generate text after each epoch\n\n","metadata":{"id":"ACSt0piO4b6p","editable":false}},{"cell_type":"code","source":"# Helper function to generate text after the end of each epoch\ndef on_epoch_end(epoch, logs):\n\tprint()\n\tprint('----- Generating text after Epoch: % d' % epoch)\n\n\t# Choosing a random starting index for the text generation\n\tstart_index = random.randint(0, len(text) - max_length - 1)\n\n\t# Sampling for different values of diversity\n\tfor diversity in [0.2, 0.5, 1.0, 1.2]:\n\t\tprint('----- diversity:', diversity)\n\n\t\tgenerated = ''\n\n\t\t# Seed sentence\n\t\tsentence = text[start_index: start_index + max_length]\n\n\t\tgenerated += sentence\n\t\tprint('----- Generating with seed: \"' + sentence + '\"')\n\t\tsys.stdout.write(generated)\n\n\t\tfor i in range(400):\n\t\t\t# Initializing the predictions vector\n\t\t\tx_pred = np.zeros((1, max_length, len(vocabulary)))\n\n\t\t\tfor t, char in enumerate(sentence):\n\t\t\t\tx_pred[0, t, char_to_indices[char]] = 1.\n\n\t\t\t# Making the predictions for the next character\n\t\t\tpreds = model.predict(x_pred, verbose = 0)[0]\n\n\t\t\t# Getting the index of the most probable next character\n\t\t\tnext_index = sample_index(preds, diversity)\n\n\t\t\t# Getting the most probable next character using the mapping built\n\t\t\tnext_char = indices_to_char[next_index]\n\n\t\t\t# Building the generated text\n\t\t\tgenerated += next_char\n\t\t\tsentence = sentence[1:] + next_char\n\n\t\t\tsys.stdout.write(next_char)\n\t\t\tsys.stdout.flush()\n\t\tprint()\n\n# Defining a custom callback function to\n# describe the internal states of the network\nprint_callback = LambdaCallback(on_epoch_end = on_epoch_end)\n","metadata":{"id":"zMZ7MNHG4cYE","execution":{"iopub.status.busy":"2024-01-01T09:03:16.409201Z","iopub.execute_input":"2024-01-01T09:03:16.409468Z","iopub.status.idle":"2024-01-01T09:03:16.423179Z","shell.execute_reply.started":"2024-01-01T09:03:16.409444Z","shell.execute_reply":"2024-01-01T09:03:16.422232Z"},"editable":false,"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"c) Helper function to save the model after each epoch in which loss decreases\n\n","metadata":{"id":"X_M0SUNl4gRz","editable":false}},{"cell_type":"code","source":"# Defining a helper function to save the model after each epoch\n# in which the loss decreases\nfilepath = \"weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor ='loss',\n\t\t\t\t\t\t\tverbose = 1, save_best_only = True,\n\t\t\t\t\t\t\tmode ='min')\n","metadata":{"id":"UAZxKXrD4gnv","execution":{"iopub.status.busy":"2024-01-01T09:03:16.424367Z","iopub.execute_input":"2024-01-01T09:03:16.424654Z","iopub.status.idle":"2024-01-01T09:03:16.437357Z","shell.execute_reply.started":"2024-01-01T09:03:16.424620Z","shell.execute_reply":"2024-01-01T09:03:16.436391Z"},"editable":false,"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"d) Helper function to reduce the learning rate each time the learning plateaus\n\n","metadata":{"id":"Uw_jKRjb4lle","editable":false}},{"cell_type":"code","source":"# Defining a helper function to reduce the learning rate each time\n# the learning plateaus\nreduce_alpha = ReduceLROnPlateau(monitor ='loss', factor = 0.2,\n\t\t\t\t\t\t\tpatience = 1, min_lr = 0.001)\ncallbacks = [print_callback, checkpoint, reduce_alpha]\n","metadata":{"id":"y1a0rS0X4l-X","execution":{"iopub.status.busy":"2024-01-01T09:03:16.438491Z","iopub.execute_input":"2024-01-01T09:03:16.438806Z","iopub.status.idle":"2024-01-01T09:03:16.447454Z","shell.execute_reply.started":"2024-01-01T09:03:16.438782Z","shell.execute_reply":"2024-01-01T09:03:16.446008Z"},"editable":false,"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Training the GRU model\n\n","metadata":{"id":"Y04B5XEW4pk9","editable":false}},{"cell_type":"code","source":"# Training the GRU model\n# model.fit(X, y, batch_size = 256, epochs = 10, callbacks = callbacks)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xnbdYwmG4p_j","outputId":"043473de-7518-46df-ec94-0ea18e52f5ba","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-01T09:03:16.448988Z","iopub.execute_input":"2024-01-01T09:03:16.449327Z","iopub.status.idle":"2024-01-01T09:03:16.456546Z","shell.execute_reply.started":"2024-01-01T09:03:16.449295Z","shell.execute_reply":"2024-01-01T09:03:16.455166Z"},"editable":false,"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Generating new and random text","metadata":{"id":"zVgf4yGt49nc","editable":false}},{"cell_type":"code","source":"# def generate_text(length, diversity):\n# \t# Get random starting text\n# \tstart_index = random.randint(0, len(text) - max_length - 1)\n\n# \t# Defining the generated text\n# \tgenerated = ''\n# \tsentence = text[start_index: start_index + max_length]\n# \tgenerated += sentence\n\n# \t# Generating new text of given length\n# \tfor i in range(length):\n\n# \t\t\t# Initializing the prediction vector\n# \t\t\tx_pred = np.zeros((1, max_length, len(vocabulary)))\n# \t\t\tfor t, char in enumerate(sentence):\n# \t\t\t\tx_pred[0, t, char_to_indices[char]] = 1.\n\n# \t\t\t# Making the predictions\n# \t\t\tpreds = model.predict(x_pred, verbose = 0)[0]\n\n# \t\t\t# Getting the index of the next most probable index\n# \t\t\tnext_index = sample_index(preds, diversity)\n\n# \t\t\t# Getting the most probable next character using the mapping built\n# \t\t\tnext_char = indices_to_char[next_index]\n\n# \t\t\t# Generating new text\n# \t\t\tgenerated += next_char\n# \t\t\tsentence = sentence[1:] + next_char\n# \treturn generated\n\n# print(generate_text(500, 1.0))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7s7WAlHEODB","outputId":"13f87fda-91ee-4477-862d-16ba0b9c523e","execution":{"iopub.status.busy":"2024-01-01T09:03:16.459779Z","iopub.execute_input":"2024-01-01T09:03:16.460056Z","iopub.status.idle":"2024-01-01T09:03:16.468752Z","shell.execute_reply.started":"2024-01-01T09:03:16.460034Z","shell.execute_reply":"2024-01-01T09:03:16.467846Z"},"editable":false,"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVFQ2X1cF08Q","outputId":"c2d3d24e-95d1-49a4-8de2-9baae741ff67","execution":{"iopub.status.busy":"2024-01-01T09:03:16.469860Z","iopub.execute_input":"2024-01-01T09:03:16.470135Z","iopub.status.idle":"2024-01-01T09:03:16.492971Z","shell.execute_reply.started":"2024-01-01T09:03:16.470112Z","shell.execute_reply":"2024-01-01T09:03:16.492117Z"},"editable":false,"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 128)               84864     \n                                                                 \n dense (Dense)               (None, 91)                11739     \n                                                                 \n activation (Activation)     (None, 91)                0         \n                                                                 \n=================================================================\nTotal params: 96603 (377.36 KB)\nTrainable params: 96603 (377.36 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model and capture the training history\nhistory = model.fit(X, y, batch_size=200, epochs=20)\n\n# Print the final accuracy\nfinal_accuracy = history.history['accuracy'][-1]\nprint(f\"Final Training Accuracy: {final_accuracy * 100:.2f}%\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Pd3S8ENQ8ED","outputId":"5d6f6e4f-19d8-4a41-8719-ab9ee9a4d1a8","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_loss = history.history['loss'][-1]\n\n# Calculate perplexity\nperplexity = np.exp(final_loss)\n\n# Print the perplexity\nprint(f\"Final Perplexity: {perplexity:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHf_hAuESMOt","outputId":"47618737-da35-4a9e-f829-5f7b6d827cda","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history.history['loss'])\npyplot.plot(history.history['accuracy'])\npyplot.title('model loss vs accuracy')\npyplot.xlabel('epoch')\npyplot.legend(['loss', 'accuracy'], loc='upper right')\npyplot.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"xTlgPx5QjbhM","outputId":"9c5561f5-0e3f-4060-b625-962e4ab9c9c0","editable":false},"execution_count":null,"outputs":[]}]}